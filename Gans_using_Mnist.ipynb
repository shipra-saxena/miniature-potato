{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gans using Mnist",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shipra-saxena/miniature-potato/blob/master/Gans_using_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nbLaw3xWbKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Reshape\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Flatten, Dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj0zMVRiWikS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(input_dim=100,units=1024,activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(input_dim=input_dim, units=units))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation))\n",
        "    model.add(Dense(128*7*7))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation))\n",
        "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
        "    model.add(UpSampling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(activation))\n",
        "    model.add(UpSampling2D((2, 2)))\n",
        "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaTDoJIDWjo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(input_shape=(28, 28, 1),nb_filter=64):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(nb_filter, (5, 5), strides=(2, 2), padding='same',\n",
        "                            input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ELU())\n",
        "    model.add(Conv2D(2*nb_filter, (5, 5), strides=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ELU())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4*nb_filter))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(ELU())\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYg3OJ5SWogu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.datasets import mnist\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-JR2d1iW06f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUM_EPOCH = 75\n",
        "LR = 0.0002  # initial learning rate\n",
        "B1 = 0.5  # momentum term\n",
        "GENERATED_IMAGE_PATH = 'images/'\n",
        "GENERATED_MODEL_PATH = 'models/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQDWpgGgW6HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    (X_train, y_train), (_, _) = mnist.load_data()\n",
        "    # normalize images\n",
        "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "\n",
        "    # build GAN\n",
        "    g = generator()\n",
        "    d = discriminator()\n",
        "\n",
        "    opt = Adam(lr=LR,beta_1=B1)\n",
        "    d.trainable = True\n",
        "    d.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              optimizer=opt)\n",
        "    d.trainable = False\n",
        "    dcgan = Sequential([g, d])\n",
        "    opt= Adam(lr=LR,beta_1=B1)\n",
        "    dcgan.compile(loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'],\n",
        "                  optimizer=opt)\n",
        "\n",
        "    num_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
        "    # create directory\n",
        "    if not os.path.exists(GENERATED_IMAGE_PATH):\n",
        "        os.mkdir(GENERATED_IMAGE_PATH)\n",
        "    if not os.path.exists(GENERATED_MODEL_PATH):\n",
        "        os.mkdir(GENERATED_MODEL_PATH)\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print(\"Total epoch:\", NUM_EPOCH, \"Number of batches:\", num_batches)\n",
        "    print(\"-------------------\")\n",
        "    z_pred = np.array([np.random.uniform(-1,1,100) for _ in range(49)])\n",
        "    y_g = [1]*BATCH_SIZE\n",
        "    y_d_true = [1]*BATCH_SIZE\n",
        "    y_d_gen = [0]*BATCH_SIZE\n",
        "    for epoch in list(map(lambda x: x+1,range(NUM_EPOCH))):\n",
        "        for index in range(num_batches):\n",
        "            X_d_true = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            X_g = np.array([np.random.normal(0,0.5,100) for _ in range(BATCH_SIZE)])\n",
        "            X_d_gen = g.predict(X_g, verbose=0)\n",
        "\n",
        "            # train discriminator\n",
        "            d_loss = d.train_on_batch(X_d_true, y_d_true)\n",
        "            d_loss = d.train_on_batch(X_d_gen, y_d_gen)\n",
        "            # train generator\n",
        "            g_loss = dcgan.train_on_batch(X_g, y_g)\n",
        "            show_progress(epoch,index,g_loss[0],d_loss[0],g_loss[1],d_loss[1])\n",
        "\n",
        "        # save generated images\n",
        "        image = combine_images(g.predict(z_pred))\n",
        "        image = image*127.5 + 127.5\n",
        "        Image.fromarray(image.astype(np.uint8))\\\n",
        "            .save(GENERATED_IMAGE_PATH+\"%03depoch.png\" % (epoch))\n",
        "        print()\n",
        "        # save models\n",
        "        g.save(GENERATED_MODEL_PATH+'dcgan_generator1.h5')\n",
        "        d.save(GENERATED_MODEL_PATH+'dcgan_discriminator.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYMchDxDXIca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def combine_images(generated_images):\n",
        "    total,width,height = generated_images.shape[:-1]\n",
        "    cols = int(math.sqrt(total))\n",
        "    rows = math.ceil(float(total)/cols)\n",
        "    combined_image = np.zeros((height*rows, width*cols),\n",
        "                              dtype=generated_images.dtype)\n",
        "\n",
        "    for index, image in enumerate(generated_images):\n",
        "        i = int(index/cols)\n",
        "        j = index % cols\n",
        "        combined_image[width*i:width*(i+1), height*j:height*(j+1)] = image[:, :, 0]\n",
        "    return combined_image\n",
        "\n",
        "def show_progress(e,i,g0,d0,g1,d1):\n",
        "    sys.stdout.write(\"\\repoch: %d, batch: %d, g_loss: %f, d_loss: %f, g_accuracy: %f, d_accuracy: %f\" % (e,i,g0,d0,g1,d1))\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74F3qwIBXGyW",
        "colab_type": "code",
        "outputId": "1af88d0c-194d-4e7c-dbab-083022aa547b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 1024)              103424    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 6272)              6428800   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 6272)              25088     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 1)         1601      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 6,768,129\n",
            "Trainable params: 6,753,409\n",
            "Non-trainable params: 14,720\n",
            "_________________________________________________________________\n",
            "None\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "elu_7 (ELU)                  (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 5, 5, 128)         204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "elu_8 (ELU)                  (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "elu_9 (ELU)                  (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 339,969\n",
            "Trainable params: 339,073\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n",
            "-------------------\n",
            "Total epoch: 75 Number of batches: 468\n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, batch: 3, g_loss: 0.677876, d_loss: 1.163836, g_accuracy: 0.671875, d_accuracy: 0.343750"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, batch: 467, g_loss: 0.840987, d_loss: 0.694515, g_accuracy: 0.460938, d_accuracy: 0.539062\n",
            "epoch: 2, batch: 467, g_loss: 0.892297, d_loss: 0.609682, g_accuracy: 0.414062, d_accuracy: 0.679688\n",
            "epoch: 3, batch: 467, g_loss: 0.820957, d_loss: 0.665932, g_accuracy: 0.445312, d_accuracy: 0.609375\n",
            "epoch: 4, batch: 467, g_loss: 0.862135, d_loss: 0.702338, g_accuracy: 0.390625, d_accuracy: 0.531250\n",
            "epoch: 5, batch: 467, g_loss: 0.740970, d_loss: 0.699278, g_accuracy: 0.492188, d_accuracy: 0.585938\n",
            "epoch: 6, batch: 467, g_loss: 0.804645, d_loss: 0.664436, g_accuracy: 0.367188, d_accuracy: 0.578125\n",
            "epoch: 7, batch: 467, g_loss: 0.789342, d_loss: 0.685812, g_accuracy: 0.406250, d_accuracy: 0.609375\n",
            "epoch: 8, batch: 467, g_loss: 0.746528, d_loss: 0.672290, g_accuracy: 0.484375, d_accuracy: 0.593750\n",
            "epoch: 9, batch: 467, g_loss: 0.787827, d_loss: 0.689776, g_accuracy: 0.382812, d_accuracy: 0.515625\n",
            "epoch: 10, batch: 467, g_loss: 0.722794, d_loss: 0.697366, g_accuracy: 0.468750, d_accuracy: 0.562500\n",
            "epoch: 11, batch: 467, g_loss: 0.728917, d_loss: 0.663109, g_accuracy: 0.445312, d_accuracy: 0.609375\n",
            "epoch: 12, batch: 467, g_loss: 0.762487, d_loss: 0.658400, g_accuracy: 0.390625, d_accuracy: 0.656250\n",
            "epoch: 13, batch: 467, g_loss: 0.757575, d_loss: 0.672480, g_accuracy: 0.429688, d_accuracy: 0.578125\n",
            "epoch: 14, batch: 467, g_loss: 0.764993, d_loss: 0.681118, g_accuracy: 0.453125, d_accuracy: 0.539062\n",
            "epoch: 15, batch: 467, g_loss: 0.735746, d_loss: 0.687931, g_accuracy: 0.468750, d_accuracy: 0.578125\n",
            "epoch: 16, batch: 467, g_loss: 0.721870, d_loss: 0.707004, g_accuracy: 0.476562, d_accuracy: 0.460938\n",
            "epoch: 17, batch: 467, g_loss: 0.735393, d_loss: 0.684635, g_accuracy: 0.429688, d_accuracy: 0.570312\n",
            "epoch: 18, batch: 467, g_loss: 0.746915, d_loss: 0.683193, g_accuracy: 0.359375, d_accuracy: 0.507812\n",
            "epoch: 19, batch: 467, g_loss: 0.732405, d_loss: 0.703053, g_accuracy: 0.468750, d_accuracy: 0.515625\n",
            "epoch: 20, batch: 467, g_loss: 0.725863, d_loss: 0.680392, g_accuracy: 0.453125, d_accuracy: 0.601562\n",
            "epoch: 21, batch: 467, g_loss: 0.734672, d_loss: 0.667615, g_accuracy: 0.453125, d_accuracy: 0.593750\n",
            "epoch: 22, batch: 467, g_loss: 0.732999, d_loss: 0.692612, g_accuracy: 0.453125, d_accuracy: 0.507812\n",
            "epoch: 23, batch: 467, g_loss: 0.745122, d_loss: 0.702059, g_accuracy: 0.406250, d_accuracy: 0.507812\n",
            "epoch: 24, batch: 467, g_loss: 0.734733, d_loss: 0.681942, g_accuracy: 0.460938, d_accuracy: 0.570312\n",
            "epoch: 25, batch: 467, g_loss: 0.700609, d_loss: 0.685833, g_accuracy: 0.492188, d_accuracy: 0.531250\n",
            "epoch: 26, batch: 467, g_loss: 0.719624, d_loss: 0.684640, g_accuracy: 0.500000, d_accuracy: 0.554688\n",
            "epoch: 27, batch: 467, g_loss: 0.706878, d_loss: 0.692755, g_accuracy: 0.429688, d_accuracy: 0.523438\n",
            "epoch: 28, batch: 467, g_loss: 0.730974, d_loss: 0.648116, g_accuracy: 0.414062, d_accuracy: 0.601562\n",
            "epoch: 29, batch: 467, g_loss: 0.741917, d_loss: 0.700366, g_accuracy: 0.406250, d_accuracy: 0.515625\n",
            "epoch: 30, batch: 467, g_loss: 0.724717, d_loss: 0.688196, g_accuracy: 0.398438, d_accuracy: 0.500000\n",
            "epoch: 31, batch: 467, g_loss: 0.728709, d_loss: 0.679982, g_accuracy: 0.476562, d_accuracy: 0.554688\n",
            "epoch: 32, batch: 467, g_loss: 0.737450, d_loss: 0.685802, g_accuracy: 0.406250, d_accuracy: 0.539062\n",
            "epoch: 33, batch: 467, g_loss: 0.728994, d_loss: 0.675076, g_accuracy: 0.460938, d_accuracy: 0.570312\n",
            "epoch: 34, batch: 467, g_loss: 0.748073, d_loss: 0.690569, g_accuracy: 0.273438, d_accuracy: 0.578125\n",
            "epoch: 35, batch: 467, g_loss: 0.720148, d_loss: 0.700489, g_accuracy: 0.445312, d_accuracy: 0.507812\n",
            "epoch: 36, batch: 467, g_loss: 0.737447, d_loss: 0.674918, g_accuracy: 0.406250, d_accuracy: 0.593750\n",
            "epoch: 37, batch: 467, g_loss: 0.713683, d_loss: 0.697950, g_accuracy: 0.484375, d_accuracy: 0.500000\n",
            "epoch: 38, batch: 467, g_loss: 0.726082, d_loss: 0.663912, g_accuracy: 0.414062, d_accuracy: 0.601562\n",
            "epoch: 39, batch: 467, g_loss: 0.734693, d_loss: 0.678808, g_accuracy: 0.421875, d_accuracy: 0.585938\n",
            "epoch: 40, batch: 467, g_loss: 0.743247, d_loss: 0.670589, g_accuracy: 0.390625, d_accuracy: 0.585938\n",
            "epoch: 41, batch: 467, g_loss: 0.743693, d_loss: 0.672991, g_accuracy: 0.312500, d_accuracy: 0.578125\n",
            "epoch: 42, batch: 467, g_loss: 0.718111, d_loss: 0.670032, g_accuracy: 0.476562, d_accuracy: 0.593750\n",
            "epoch: 43, batch: 467, g_loss: 0.725209, d_loss: 0.676089, g_accuracy: 0.406250, d_accuracy: 0.570312\n",
            "epoch: 44, batch: 467, g_loss: 0.708926, d_loss: 0.667203, g_accuracy: 0.484375, d_accuracy: 0.585938\n",
            "epoch: 45, batch: 467, g_loss: 0.736559, d_loss: 0.697403, g_accuracy: 0.406250, d_accuracy: 0.546875\n",
            "epoch: 46, batch: 467, g_loss: 0.720706, d_loss: 0.678359, g_accuracy: 0.429688, d_accuracy: 0.539062\n",
            "epoch: 47, batch: 467, g_loss: 0.728722, d_loss: 0.690032, g_accuracy: 0.445312, d_accuracy: 0.554688\n",
            "epoch: 48, batch: 467, g_loss: 0.726971, d_loss: 0.693515, g_accuracy: 0.484375, d_accuracy: 0.500000\n",
            "epoch: 49, batch: 467, g_loss: 0.716448, d_loss: 0.699495, g_accuracy: 0.468750, d_accuracy: 0.507812\n",
            "epoch: 50, batch: 467, g_loss: 0.716913, d_loss: 0.662197, g_accuracy: 0.460938, d_accuracy: 0.656250\n",
            "epoch: 51, batch: 467, g_loss: 0.747825, d_loss: 0.673697, g_accuracy: 0.367188, d_accuracy: 0.601562\n",
            "epoch: 52, batch: 467, g_loss: 0.737658, d_loss: 0.650546, g_accuracy: 0.445312, d_accuracy: 0.671875\n",
            "epoch: 53, batch: 467, g_loss: 0.754034, d_loss: 0.714687, g_accuracy: 0.406250, d_accuracy: 0.500000\n",
            "epoch: 54, batch: 467, g_loss: 0.723246, d_loss: 0.704150, g_accuracy: 0.429688, d_accuracy: 0.546875\n",
            "epoch: 55, batch: 467, g_loss: 0.744285, d_loss: 0.679825, g_accuracy: 0.398438, d_accuracy: 0.617188\n",
            "epoch: 56, batch: 467, g_loss: 0.718710, d_loss: 0.660316, g_accuracy: 0.429688, d_accuracy: 0.617188\n",
            "epoch: 57, batch: 467, g_loss: 0.729627, d_loss: 0.669721, g_accuracy: 0.437500, d_accuracy: 0.593750\n",
            "epoch: 58, batch: 467, g_loss: 0.725178, d_loss: 0.679062, g_accuracy: 0.460938, d_accuracy: 0.546875\n",
            "epoch: 59, batch: 467, g_loss: 0.711458, d_loss: 0.690302, g_accuracy: 0.492188, d_accuracy: 0.562500\n",
            "epoch: 60, batch: 467, g_loss: 0.717147, d_loss: 0.678912, g_accuracy: 0.492188, d_accuracy: 0.578125\n",
            "epoch: 61, batch: 467, g_loss: 0.715411, d_loss: 0.692434, g_accuracy: 0.460938, d_accuracy: 0.500000\n",
            "epoch: 62, batch: 467, g_loss: 0.717916, d_loss: 0.684544, g_accuracy: 0.476562, d_accuracy: 0.523438\n",
            "epoch: 63, batch: 467, g_loss: 0.741859, d_loss: 0.692196, g_accuracy: 0.398438, d_accuracy: 0.546875\n",
            "epoch: 64, batch: 467, g_loss: 0.761841, d_loss: 0.651226, g_accuracy: 0.406250, d_accuracy: 0.648438\n",
            "epoch: 65, batch: 467, g_loss: 0.709555, d_loss: 0.694935, g_accuracy: 0.476562, d_accuracy: 0.539062\n",
            "epoch: 66, batch: 467, g_loss: 0.717367, d_loss: 0.686451, g_accuracy: 0.421875, d_accuracy: 0.546875\n",
            "epoch: 67, batch: 467, g_loss: 0.739302, d_loss: 0.651920, g_accuracy: 0.398438, d_accuracy: 0.695312\n",
            "epoch: 68, batch: 467, g_loss: 0.755428, d_loss: 0.675382, g_accuracy: 0.375000, d_accuracy: 0.570312\n",
            "epoch: 69, batch: 467, g_loss: 0.720261, d_loss: 0.687103, g_accuracy: 0.484375, d_accuracy: 0.562500\n",
            "epoch: 70, batch: 467, g_loss: 0.733476, d_loss: 0.688481, g_accuracy: 0.429688, d_accuracy: 0.484375\n",
            "epoch: 71, batch: 467, g_loss: 0.753444, d_loss: 0.663236, g_accuracy: 0.414062, d_accuracy: 0.617188\n",
            "epoch: 72, batch: 467, g_loss: 0.742047, d_loss: 0.693479, g_accuracy: 0.484375, d_accuracy: 0.476562\n",
            "epoch: 73, batch: 467, g_loss: 0.719010, d_loss: 0.699573, g_accuracy: 0.460938, d_accuracy: 0.531250\n",
            "epoch: 74, batch: 467, g_loss: 0.750146, d_loss: 0.683326, g_accuracy: 0.437500, d_accuracy: 0.523438\n",
            "epoch: 75, batch: 467, g_loss: 0.759977, d_loss: 0.661640, g_accuracy: 0.351562, d_accuracy: 0.593750\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}